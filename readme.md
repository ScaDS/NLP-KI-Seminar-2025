# Seminar Processing Natural Language using Artificial Intelligence (NLP-KI Seminar)

This semester we will dive into topics centered around different prompt engineering techniques. 
You can get an overview about the topic in [Sahoo et al (2024)](http://arxiv.org/abs/2402.07927v2).

Details of the seminar are explained in [this slide deck](NLP-KI_Seminar_Intro.pdf).

## Available topics

* [Hyojin Bahng et al. 'Exploring Visual Prompts for Adapting Large-Scale Models' ](https://arxiv.org/abs/2203.17274v2) 
* [Tom B. Brown et al. 'Language Models are Few-Shot Learners' ](https://arxiv.org/abs/2005.14165v4) 
* [Wenhu Chen et al. 'Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks' ](https://arxiv.org/abs/2211.12588v4) 
* [Yew Ken Chia et al. 'Contrastive Chain-of-Thought Prompting' ](https://arxiv.org/abs/2311.09277v1) 
* [Yihe Deng et al. 'Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves' ](https://arxiv.org/abs/2311.04205v2) 
* [Linger Deng et al. 'R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models' ](https://arxiv.org/abs/2410.17885v2) 
* [Shehzaad Dhuliawala et al. 'Chain-of-Verification Reduces Hallucination in Large Language Models' ](https://arxiv.org/abs/2309.11495v2) 
* [Shizhe Diao et al. 'Active Prompting with Chain-of-Thought for Large Language Models' ](https://arxiv.org/abs/2302.12246v5) 
* [Wachara Fungwacharakorn et al. 'Layer-of-Thoughts Prompting (LoT): Leveraging LLM-Based Retrieval with Constraint Hierarchies' ](https://arxiv.org/abs/2410.12153v1) 
* [Hanxu Hu et al. 'Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models' ](https://arxiv.org/abs/2305.10276v7) 
* [Patrick Lewis et al. 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks' ](https://arxiv.org/abs/2005.11401v4) 
* [Cheng Li et al. 'Large Language Models Understand and Can be Enhanced by Emotional Stimuli' ](https://arxiv.org/abs/2307.11760v7) 
* [Chengshu Li et al. 'Chain of Code: Reasoning with a Language Model-Augmented Code Emulator' ](https://arxiv.org/abs/2312.04474v4) 
* [Jia Li et al. 'Structured Chain-of-Thought Prompting for Code Generation' ](https://arxiv.org/abs/2305.06599v3) 
* [Xingxuan Li et al. 'Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources' ](https://arxiv.org/abs/2305.13269v4)
* [Tongxuan Liu et al. 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models' ](https://arxiv.org/abs/2409.17539v2) 
* [Jieyi Long et al. 'Large Language Model Guided Tree-of-Thought' ](https://arxiv.org/abs/2305.08291v1) 
* [Aman Madaan et al. 'Self-Refine: Iterative Refinement with Self-Feedback' ](https://arxiv.org/abs/2303.17651v2)
* [Rajasekhar Reddy Mekala et al. 'EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning' ](https://arxiv.org/abs/2309.10687v3)
* [Maxwell Nye et al. 'Show Your Work: Scratchpads for Intermediate Computation with Language Models' ](https://arxiv.org/abs/2112.00114v1)
* [Bhargavi Paranjape et al. 'ART: Automatic multi-step reasoning and tool-use for large language models' ](https://arxiv.org/abs/2303.09014v1) 
* [Haritz Puerto et al. 'Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs' ](https://arxiv.org/abs/2401.10065v3)
* [Xuezhi Wang et al. 'Self-Consistency Improves Chain of Thought Reasoning in Language Models' ](https://arxiv.org/abs/2203.11171v4)
* [Zilong Wang et al. 'Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding' ](https://arxiv.org/abs/2401.04398v2) 
* [Jason Wei et al. 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models' ](https://arxiv.org/abs/2201.11903v6) 
* [Jason Weston et al. 'System 2 Attention (is something you might need too)' ](https://arxiv.org/abs/2311.11829v1) 
* [Silei Xu et al. 'Chain of Draft: Thinking Faster by Writing Less' ](https://arxiv.org/abs/2502.18600v2) 
* [Chengrun Yang et al. 'Large Language Models as Optimizers' ](https://arxiv.org/abs/2309.03409v3) 
* [Ling Yang et al. 'Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models' ](https://arxiv.org/abs/2406.04271v2) 
* [Shunyu Yao et al. 'ReAct: Synergizing Reasoning and Acting in Language Models' ](https://arxiv.org/abs/2210.03629v3) 
* [Shunyu Yao et al. 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models' ](https://arxiv.org/abs/2305.10601v2) 
* [Yao Yao et al. 'Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models' ](https://arxiv.org/abs/2305.16582v2) 
* [Wenhao Yu et al. 'Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models' ](https://arxiv.org/abs/2311.09210v2) 
* [Xiaosong Yuan et al. 'Instance-adaptive Zero-shot Chain-of-Thought Prompting' ](https://arxiv.org/abs/2409.20441v3) 
* [Zhuosheng Zhang et al. 'Automatic Chain of Thought Prompting in Large Language Models' ](https://arxiv.org/abs/2210.03493v1) 
* [Xinliang Frederick Zhang et al. 'Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives'](https://arxiv.org/abs/2410.05558v2) 
* [Xufeng Zhao et al. 'Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic' ](https://arxiv.org/abs/2309.13339v4) 
* [Huaixiu Steven Zheng et al. 'Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models' ](https://arxiv.org/abs/2310.06117v2) 
* [Yongchao Zhou et al. 'Large Language Models Are Human-Level Prompt Engineers' ](https://arxiv.org/abs/2211.01910v2) 
* [Yucheng Zhou et al. 'Thread of Thought Unraveling Chaotic Contexts' ](https://arxiv.org/abs/2311.08734v1) 
* [Zhanke Zhou et al. 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?' ](https://arxiv.org/abs/2410.23856v1) 


## List of speakers / topics

This list will be published April 15th 2025.

* 11.4.2025: ? and ? 
* 25.4.2025: ? and ? 
* 2.5.2025: ? and ? 
* 9.5.2025: ? and ? 
* 16.5.2025: ? and ? 
* 23.5.2025: ? and ? 
* 30.5.2025: ? and ? 
* 6.6.2025: ? and ? 
* 13.6.2025: ? and ? 
* 20.6.2025: ? and ? 
* 27.6.2025: ? and ? 
* 4.7.2025: ? and ? 
* 11.7.2025: ? and ? 


